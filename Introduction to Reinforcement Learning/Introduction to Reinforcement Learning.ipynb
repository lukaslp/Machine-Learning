{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "\n",
    "## Content\n",
    "\n",
    "### 1. Definition\n",
    "### 2. Reward Hypothesis\n",
    "### 3. Tasks\n",
    "### 4. Approaches of Reinforcement Learning\n",
    "### 5. Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "\n",
    "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem. To get the machine to do what the programmer wants, the artificial intelligence gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward.\n",
    "\n",
    "Although the designer sets the reward policy–that is, the rules of the game–he gives the model no hints or suggestions for how to solve the game. It’s up to the model to figure out how to perform the task to maximize the reward, starting from totally random trials and finishing with sophisticated tactics and superhuman skills.\n",
    "\n",
    "Reinforcement Learning is a lot based on the human learning process. Take this as an example: a boy who never saw fire before approaches a fireplace. He them feels warmth and feels good, as his country goes through a severe winter. The kid understand that fire is a positive thing and gets a **positive reward**. However, the boy try to touch the fire and burns his hand, getting therefore a **negative reward**. The kid just understood that fire is positive when the distance is sufficient, because it produces warmth but when he gets to close to it, he will be burned. Thus, Reinforcement Learning is just a computational approach of learning from action, imitating the way that humans learn.\n",
    "\n",
    "<img src='Data/comparation.png'/>\n",
    "\n",
    "### The Reinforcement Learning Process\n",
    "\n",
    "As an example, imagine an agent learning to play Super Mario Bros.\n",
    "\n",
    "<img src='Data/mario.png'/>\n",
    "\n",
    "The Reinforcement Learning (RL) process can be modeled as a loop that works like this:\n",
    "\n",
    "- The agent recaives **state S0** from the **Environment** (first frame of the game - which is the state - from the Super Mario Bros - which is the environment);\n",
    "- Based on that **state S0**, the agent takes an **action A0** (move right);\n",
    "- Environment transitions to a **new state S1** (new frame);\n",
    "- Environment gives some **reward R1** to the agent (not dead: +1).\n",
    "\n",
    "This RL loop outputs a squence of **state, action and reward.** Hence, the goal of the agent is to maximize the expected cumulative reward.\n",
    "\n",
    "It is very interesting to note that this could also be made through a Supervised Learning approach. You could put an agent to learn from a really good Super Mario Bros player and imitate its actions through some algorithm given that player's output. However, by definition, the agent couldn't play the game better than the player used as sample data.\n",
    "\n",
    "### The basic concept of RL\n",
    "\n",
    "Therefore, the RL setup is composed of these 2 main components, an agent and an environment.\n",
    "\n",
    "<img src='Data/setup.png'/>\n",
    "\n",
    "- 1. **Agent:** a hypothetical entity which performs actions in an environment to gain some reward;\n",
    "- 2. **Action (a):** all the possible moves that the agent can take;\n",
    "- 3. **Environment (e):** a scenario the agent has to face;\n",
    "- 4. **State (s):** current situation returned by the environment;\n",
    "- 5. **Reward (R):** an immediate return sent back from the environment to evaluate the last action by the agent;\n",
    "- 6. **Policy (π):** the strategy that the agent employs to determine next action based on the current state;\n",
    "- 7. **Value (V):** the expected long-term return with discount, as opposed to the short-term reward. Vπ(s), is defined as the expected long-term return of the current state s under policy π;\n",
    "- 8. **Q-value or action-value (Q):** Q-value is similar to Value, except that it takes an extra parameter, the current action a. Qπ(s,a) refers to the long-term return of the current state s, taking action a under policy π.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reward Hypothesis\n",
    "\n",
    "The goal of the agent is to maximize the expected cumulative reward. All goals can be described by the maximization of the expected cumulative reward.\n",
    "\n",
    "The cumulative reward at each time step **t** can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ G_t=R_{t+1}+R_{t+2}+...\n",
    "\\end{equation*}\n",
    "\n",
    "Which is equivalent to:\n",
    "\\begin{equation*}\n",
    "\\ G_t=\\sum_{k=0}^T R_{t+k+1}\n",
    "\\end{equation*}\n",
    "\n",
    "In reality we can't just add the rewards like that though. The rewards that come sooner (in the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "<img src='Data/mouse_game_01.png'/>\n",
    "\n",
    "This mouse is the agent and the opponent is the cat. The goal is to eat the maximum amount of cheese before being eaten by the cat. As we can see in the diagram, it's more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).\n",
    "\n",
    "As a consequence, the reward near the cat, even if it is bigger (more cheese), will be discounted. The agente is not really sure he'll be able to eat it. To discount the rewards, we can define a discount rate called gamma between 0 and 1. The larger the gamma, the smaller the discount, i.e. the learning agent cares more about the long term reward. The smaller the gamma, the bigger the discount and the agent cares more about the short term reward.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ G_t=\\sum_{k=0}^{\\infty} {\\gamma}^k R_{t+k+1} \\\\\n",
    "\\ G_t=R_{t+1}+{\\gamma}R_{t+2}+{\\gamma}^2R_{t+3}...\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/\n",
    "- https://towardsdatascience.com/an-introduction-to-reinforcement-learning-1e7825c60bbe\n",
    "- https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
